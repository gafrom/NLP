{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RNN LSTM for text classification\n",
    "=============\n",
    "<span style=\"color: lightsteelblue;\">Deep Learning</span>\n",
    "\n",
    "The goal of this notebook is to train recurrent neural network with LSTM cell for the purpose of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# import re\n",
    "# import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# from six.moves import range\n",
    "# from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "# custom library\n",
    "from nlp.preparer import Corpus\n",
    "from nlp.iterator import BatchIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus label: good,  length: 6666 articles,  av length: 321 words,  max length: 4977 words.\n",
      "Corpus raw article: 3 февраля в большинстве европейских стран закрылось зимнее трансферное окно — период, когда клубы могут заявлять новых футболистов, купленных у других команд. Ценники меняются чуть ли не ежемесячно. Н\n",
      "Corpus data (words): ['num', 'февраля', 'в', 'большинстве', 'европейских', 'стран', 'закрылось', 'зимнее', 'трансферное', 'окно', 'период', ',', 'когда', 'клубы', 'могут', 'заявлять', 'новых', 'футболистов', ',', 'купленных']\n",
      "\n",
      "Corpus label: bad,  length: 7519 articles,  av length: 84 words,  max length: 594 words.\n",
      "Corpus raw article: Неизвестный угрожает взорвать аэропорт Кишинева, если ему не дадут миллион. Неизвестный сообщил о бомбе в аэропорту Международного аэропорта Кишинева и требует миллион рублей, сообщили в пограничной п\n",
      "Corpus data (words): ['неизвестный', 'угрожает', 'взорвать', 'аэропорт', 'кишинева', ',', 'если', 'ему', 'не', 'дадут', 'миллион', '.', 'неизвестный', 'сообщил', 'о', 'бомбе', 'в', 'аэропорту', 'международного', 'аэропорта']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpora_paths = ['./articles/good.articles', './articles/bad.articles'] \n",
    "corpora = []\n",
    "lengths = []\n",
    "\n",
    "for path in corpora_paths:\n",
    "  corpus = Corpus(path)\n",
    "  corpora.append(corpus)\n",
    "  length = [len(article) for article in corpus.articles]\n",
    "  lengths.append(length)\n",
    "\n",
    "  print(f\"Corpus label: {corpus.label}, \",\n",
    "        f\"length: {len(corpus.articles)} articles, \",\n",
    "        f\"av length: {round(corpus.average_length())} words, \",\n",
    "        f\"max length: {max(length)} words.\")\n",
    "  print(f\"Corpus raw article: {corpus.raw[0][:200]}\")\n",
    "  print(f\"Corpus data (words): {corpus.articles[0][:20]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing articles lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4UAAAHwCAYAAAARoMr7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiSNMZNCSZIkSRpj/wcCLrrKUpX/bAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114247128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization\n",
    "plt.figure().set_size_inches(15, 8)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.xlabel('Article length (words)')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.title(f'Lengths distribution: {corpora[0].label} articles')\n",
    "plt.hist(lengths[0], 200)\n",
    "\n",
    "plt.subplots_adjust(hspace=.5)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xlabel('Article length (words)')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.title(f'Lengths distribution: {corpora[1].label} articles')\n",
    "plt.hist(lengths[1], 200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top popular words counts: [['UNK', 311785], (',', 191060), ('.', 167323), ('в', 121865), ('num', 70531), ('и', 59437), ('на', 47376), ('с', 26509), ('что', 22411), ('по', 21883), ('не', 20173), ('из', 16359), (')', 12317), ('(', 12286), ('как', 11494)]\n",
      "\n",
      "Example word: 23732 => слон\n",
      "\n",
      "Example data: [4, 344, 3, 3705, 2011, 282, 0, 10653, 0, 4102, 586, 1, 67, 5320, 152, 0, 448, 8138, 1, 22380]\n",
      "\n",
      "Validation sizes split train / validation: 6646 / 20\n"
     ]
    }
   ],
   "source": [
    "validation_size = 20 # articles per each class\n",
    "vocabulary_size = 25000\n",
    "\n",
    "def build_dictionary(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)  \n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "\n",
    "  return dictionary, reverse_dictionary, count\n",
    "\n",
    "def build_dataset(words, dictionary, count):\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "\n",
    "  count[0][1] = count[0][1] + unk_count\n",
    "\n",
    "  return data\n",
    "\n",
    "# flatten all words into a single bag\n",
    "all_words = [word for corpus in corpora for words in corpus.articles for word in words]\n",
    "\n",
    "dictionary, reverse_dictionary, count = build_dictionary(all_words)\n",
    "good_data = [build_dataset(article, dictionary, count) for article in corpora[0].articles]\n",
    "bad_data = [build_dataset(article, dictionary, count) for article in corpora[1].articles]\n",
    "\n",
    "# Splitting between train and validation datasets\n",
    "good_train_data = good_data[:-validation_size]\n",
    "good_validation_data = good_data[-validation_size:]\n",
    "bad_train_data = bad_data[:-validation_size]\n",
    "bad_validation_data = bad_data[-validation_size:]\n",
    "\n",
    "print(f\"Top popular words counts: {count[:15]}\\n\")\n",
    "print(f\"Example word: {dictionary['слон']} => {reverse_dictionary[dictionary['слон']]}\\n\")\n",
    "print(f'Example data: {good_data[0][:20]}\\n')\n",
    "print(f'Validation sizes split train / validation: {len(good_train_data)} / {len(good_validation_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** on how `BatchIterator` generates a training batch for the LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1995, 0, 2, 190, 6285, 3, 0, 0, 15, 4, 201, 290, 1, 228, 3, 5438, 0, 15, 4], [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0]], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n",
      "[[201, 2, 1185, 3249, 6873, 1502, 0, 1, 75, 157, 10, 8778, 1533, 2, 1185, 103, 18, 9826, 3, 724], [[1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1]], [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "good = [1, 0] # Representation of first class\n",
    "bad  = [0, 1] # Representation of second class\n",
    "\n",
    "batches = BatchIterator(batch_size, good_data, bad_data, good, bad)\n",
    "\n",
    "for i in range(20):\n",
    "  batch = batches.next()\n",
    "  if i > 17: print(batch)\n",
    "\n",
    "batches.rewind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "num_classes = 2 # Number of classes among which to predict\n",
    "embedding_size = 128 # Dimension of a word vector\n",
    "num_unrollings = 20 # Number of recurrent steps\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([1, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, num_classes], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([num_classes]))\n",
    "  # Embeddings\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs         = tf.placeholder(tf.int32, shape=[num_unrollings])\n",
    "  train_state_cleaners = tf.placeholder(tf.float32, shape=[num_unrollings])\n",
    "  train_labels         = tf.placeholder(tf.int32, shape=[num_unrollings, num_classes])\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  lstm_outputs = list()\n",
    "  output = saved_output\n",
    "  state  = saved_state\n",
    "  lstm_inputs = tf.unstack(tf.nn.embedding_lookup(embeddings, train_inputs)) # ~ (num_unrollings, embedding_size)\n",
    "  lstm_state_cleaners = tf.unstack(train_state_cleaners)\n",
    "\n",
    "  for lstm_input, state_cleaner in zip(lstm_inputs, lstm_state_cleaners):\n",
    "    reshaped_lstm_input = tf.reshape(lstm_input, (1, embedding_size))\n",
    "    output, state = lstm_cell(reshaped_lstm_input,\n",
    "                              tf.multiply(state_cleaner, output),\n",
    "                              tf.multiply(state_cleaner, state))\n",
    "    lstm_outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(lstm_outputs, 0), w, b) # ~ (num_unrollings, num_classes)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      labels=train_labels,\n",
    "      logits=logits\n",
    "    ))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.5, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "#   # Sampling and validation eval: batch 1, no unrolling.\n",
    "#   sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "#   saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "#   saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "#   reset_sample_state = tf.group(\n",
    "#     saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "#     saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "#   sample_output, sample_state = lstm_cell(\n",
    "#     sample_input, saved_sample_output, saved_sample_state)\n",
    "#   with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "#                                 saved_sample_state.assign(sample_state)]):\n",
    "#     sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 0.749262 learning rate: 10.000000\n",
      "Average loss at step 250: 0.497129 learning rate: 10.000000\n",
      "Average loss at step 500: 0.452107 learning rate: 10.000000\n",
      "Average loss at step 750: 0.430498 learning rate: 10.000000\n",
      "Average loss at step 1000: 0.414900 learning rate: 10.000000\n",
      "Average loss at step 1250: 0.528327 learning rate: 10.000000\n",
      "Average loss at step 1500: 0.565812 learning rate: 10.000000\n",
      "Average loss at step 1750: 0.519341 learning rate: 10.000000\n",
      "Average loss at step 2000: 0.567336 learning rate: 10.000000\n",
      "Average loss at step 2250: 0.502500 learning rate: 10.000000\n",
      "Average loss at step 2500: 0.361652 learning rate: 10.000000\n",
      "Average loss at step 2750: 0.392637 learning rate: 10.000000\n",
      "Average loss at step 3000: 0.664571 learning rate: 10.000000\n",
      "Average loss at step 3250: 0.399711 learning rate: 10.000000\n",
      "Average loss at step 3500: 0.482922 learning rate: 10.000000\n",
      "Average loss at step 3750: 0.740784 learning rate: 10.000000\n",
      "Average loss at step 4000: 0.495342 learning rate: 10.000000\n",
      "Average loss at step 4250: 0.720901 learning rate: 10.000000\n",
      "Average loss at step 4500: 0.685608 learning rate: 10.000000\n",
      "Average loss at step 4750: 1.071191 learning rate: 10.000000\n",
      "Average loss at step 5000: 0.926332 learning rate: 5.000000\n",
      "Average loss at step 5250: 0.592140 learning rate: 5.000000\n",
      "Average loss at step 5500: 0.676400 learning rate: 5.000000\n",
      "Average loss at step 5750: 0.566429 learning rate: 5.000000\n",
      "Average loss at step 6000: 0.690230 learning rate: 5.000000\n",
      "Average loss at step 6250: 0.647271 learning rate: 5.000000\n",
      "Average loss at step 6500: 0.571799 learning rate: 5.000000\n",
      "Average loss at step 6750: 0.348592 learning rate: 5.000000\n",
      "Average loss at step 7000: 0.461480 learning rate: 5.000000\n",
      "Average loss at step 7250: 0.269565 learning rate: 5.000000\n",
      "Average loss at step 7500: 0.458006 learning rate: 5.000000\n",
      "Average loss at step 7750: 0.416811 learning rate: 5.000000\n",
      "Average loss at step 8000: 0.392263 learning rate: 5.000000\n",
      "Average loss at step 8250: 0.518840 learning rate: 5.000000\n",
      "Average loss at step 8500: 0.434761 learning rate: 5.000000\n",
      "Average loss at step 8750: 0.198802 learning rate: 5.000000\n",
      "Average loss at step 9000: 0.507880 learning rate: 5.000000\n",
      "Average loss at step 9250: 0.470914 learning rate: 5.000000\n",
      "Average loss at step 9500: 0.517115 learning rate: 5.000000\n",
      "Average loss at step 9750: 0.632027 learning rate: 5.000000\n",
      "Average loss at step 10000: 0.564797 learning rate: 2.500000\n",
      "Average loss at step 10250: 0.471965 learning rate: 2.500000\n",
      "Average loss at step 10500: 0.501233 learning rate: 2.500000\n",
      "Average loss at step 10750: 0.374139 learning rate: 2.500000\n",
      "Average loss at step 11000: 0.515648 learning rate: 2.500000\n",
      "Average loss at step 11250: 0.360156 learning rate: 2.500000\n",
      "Average loss at step 11500: 0.481673 learning rate: 2.500000\n",
      "Average loss at step 11750: 0.373528 learning rate: 2.500000\n",
      "Average loss at step 12000: 0.359464 learning rate: 2.500000\n",
      "Average loss at step 12250: 0.217443 learning rate: 2.500000\n",
      "Average loss at step 12500: 0.345119 learning rate: 2.500000\n",
      "Average loss at step 12750: 0.565806 learning rate: 2.500000\n",
      "Average loss at step 13000: 0.327018 learning rate: 2.500000\n",
      "Average loss at step 13250: 0.341992 learning rate: 2.500000\n",
      "Average loss at step 13500: 0.504519 learning rate: 2.500000\n",
      "Average loss at step 13750: 0.354596 learning rate: 2.500000\n",
      "Average loss at step 14000: 0.473339 learning rate: 2.500000\n",
      "Average loss at step 14250: 0.415113 learning rate: 2.500000\n",
      "Average loss at step 14500: 0.314548 learning rate: 2.500000\n",
      "Average loss at step 14750: 0.366480 learning rate: 2.500000\n",
      "Average loss at step 15000: 0.462706 learning rate: 1.250000\n",
      "Average loss at step 15250: 0.340851 learning rate: 1.250000\n",
      "Average loss at step 15500: 0.346897 learning rate: 1.250000\n",
      "Average loss at step 15750: 0.469701 learning rate: 1.250000\n",
      "Average loss at step 16000: 0.265754 learning rate: 1.250000\n",
      "Average loss at step 16250: 0.547027 learning rate: 1.250000\n",
      "Average loss at step 16500: 0.318712 learning rate: 1.250000\n",
      "Average loss at step 16750: 0.397710 learning rate: 1.250000\n",
      "Average loss at step 17000: 0.467084 learning rate: 1.250000\n",
      "Average loss at step 17250: 0.296452 learning rate: 1.250000\n",
      "Average loss at step 17500: 0.319510 learning rate: 1.250000\n",
      "Average loss at step 17750: 0.270644 learning rate: 1.250000\n",
      "Average loss at step 18000: 0.354528 learning rate: 1.250000\n",
      "Average loss at step 18250: 0.210927 learning rate: 1.250000\n",
      "Average loss at step 18500: 0.194804 learning rate: 1.250000\n",
      "Average loss at step 18750: 0.354565 learning rate: 1.250000\n",
      "Average loss at step 19000: 0.477979 learning rate: 1.250000\n",
      "Average loss at step 19250: 0.422189 learning rate: 1.250000\n",
      "Average loss at step 19500: 0.166561 learning rate: 1.250000\n",
      "Average loss at step 19750: 0.443915 learning rate: 1.250000\n",
      "Average loss at step 20000: 0.399138 learning rate: 0.625000\n",
      "Average loss at step 20250: 0.411418 learning rate: 0.625000\n",
      "Average loss at step 20500: 0.424893 learning rate: 0.625000\n",
      "Average loss at step 20750: 0.291343 learning rate: 0.625000\n",
      "Average loss at step 21000: 0.276877 learning rate: 0.625000\n",
      "Average loss at step 21250: 0.398273 learning rate: 0.625000\n",
      "Average loss at step 21500: 0.339886 learning rate: 0.625000\n",
      "Average loss at step 21750: 0.327170 learning rate: 0.625000\n",
      "Average loss at step 22000: 0.366128 learning rate: 0.625000\n",
      "Average loss at step 22250: 0.293876 learning rate: 0.625000\n",
      "Average loss at step 22500: 0.343739 learning rate: 0.625000\n",
      "Average loss at step 22750: 0.258760 learning rate: 0.625000\n",
      "Average loss at step 23000: 0.236054 learning rate: 0.625000\n",
      "Average loss at step 23250: 0.356958 learning rate: 0.625000\n",
      "Average loss at step 23500: 0.302969 learning rate: 0.625000\n",
      "Average loss at step 23750: 0.236253 learning rate: 0.625000\n",
      "Average loss at step 24000: 0.222301 learning rate: 0.625000\n",
      "Average loss at step 24250: 0.295557 learning rate: 0.625000\n",
      "Average loss at step 24500: 0.180020 learning rate: 0.625000\n",
      "Average loss at step 24750: 0.412280 learning rate: 0.625000\n",
      "Average loss at step 25000: 0.259179 learning rate: 0.312500\n",
      "Average loss at step 25250: 0.383316 learning rate: 0.312500\n",
      "Average loss at step 25500: 0.214293 learning rate: 0.312500\n",
      "Average loss at step 25750: 0.238606 learning rate: 0.312500\n",
      "Average loss at step 26000: 0.314844 learning rate: 0.312500\n",
      "Average loss at step 26250: 0.198003 learning rate: 0.312500\n",
      "Average loss at step 26500: 0.177480 learning rate: 0.312500\n",
      "Average loss at step 26750: 0.204370 learning rate: 0.312500\n",
      "Average loss at step 27000: 0.261767 learning rate: 0.312500\n",
      "Average loss at step 27250: 0.240613 learning rate: 0.312500\n",
      "Average loss at step 27500: 0.103192 learning rate: 0.312500\n",
      "Average loss at step 27750: 0.116831 learning rate: 0.312500\n",
      "Average loss at step 28000: 0.218361 learning rate: 0.312500\n",
      "Average loss at step 28250: 0.105977 learning rate: 0.312500\n",
      "Average loss at step 28500: 0.184786 learning rate: 0.312500\n",
      "Average loss at step 28750: 0.171280 learning rate: 0.312500\n",
      "Average loss at step 29000: 0.169693 learning rate: 0.312500\n",
      "Average loss at step 29250: 0.149569 learning rate: 0.312500\n",
      "Average loss at step 29500: 0.250956 learning rate: 0.312500\n",
      "Average loss at step 29750: 0.218349 learning rate: 0.312500\n",
      "Average loss at step 30000: 0.089070 learning rate: 0.156250\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "summary_frequency = 250\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch = batches.next()\n",
    "    feed_dict = { train_inputs: batch[0],\n",
    "                  train_labels: batch[1],\n",
    "                  train_state_cleaners: batch[2] }\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "\n",
    "#       labels = np.concatenate(list(batches)[1:])\n",
    "#       print('Minibatch perplexity: %.2f' % float(\n",
    "#         np.exp(logprob(predictions, labels))))\n",
    "#       if step % (summary_frequency * 10) == 0:\n",
    "#         # Generate some samples.\n",
    "#         print('=' * 80)\n",
    "#         for _ in range(5):\n",
    "#           feed = sample(random_distribution())\n",
    "#           sentence = characters(feed)[0]\n",
    "#           reset_sample_state.run()\n",
    "#           for _ in range(79):\n",
    "#             prediction = sample_prediction.eval({sample_input: feed})\n",
    "#             feed = sample(prediction)\n",
    "#             sentence += characters(feed)[0]\n",
    "#           print(sentence)\n",
    "#         print('=' * 80)\n",
    "#       # Measure validation set perplexity.\n",
    "#       reset_sample_state.run()\n",
    "#       valid_logprob = 0\n",
    "#       for _ in range(valid_size):\n",
    "#         b = valid_batches.next()\n",
    "#         predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "#         valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "#       print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "#         valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
