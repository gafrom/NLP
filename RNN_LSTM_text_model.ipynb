{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN LSTM for text classification\n",
    "=============\n",
    "<span style=\"color: lightsteelblue;\">Deep Learning</span>\n",
    "\n",
    "The goal of this notebook is to train recurrent neural network with LSTM cell for the purpose of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# import re\n",
    "# import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# from six.moves import range\n",
    "# from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "# custom library\n",
    "from nlp.preparer import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus label: good,  length: 6666 articles,  av length: 321 words,  max length: 4977 words.\n",
      "Corpus raw article: 3 февраля в большинстве европейских стран закрылось зимнее трансферное окно — период, когда клубы могут заявлять новых футболистов, купленных у других команд. Ценники меняются чуть ли не ежемесячно. На их формирование влияет множество факторов: результативность, желание клубов выгодно заработать, а \n",
      "Corpus data (words): ['num', 'февраля', 'в', 'большинстве', 'европейских', 'стран', 'закрылось', 'зимнее', 'трансферное', 'окно', 'период', ',', 'когда', 'клубы', 'могут', 'заявлять', 'новых', 'футболистов', ',', 'купленных', 'у', 'других', 'команд', '.', 'ценники', 'меняются', 'чуть', 'ли', 'не', 'ежемесячно', '.', 'на', 'их', 'формирование', 'влияет', 'множество', 'факторов', 'результативность', ',', 'желание', 'клубов', 'выгодно', 'заработать', ',', 'а', 'также', 'личные', 'амбиции', 'спортсменов', '.']\n",
      "\n",
      "Corpus label: bad,  length: 7519 articles,  av length: 84 words,  max length: 594 words.\n",
      "Corpus raw article: Неизвестный угрожает взорвать аэропорт Кишинева, если ему не дадут миллион. Неизвестный сообщил о бомбе в аэропорту Международного аэропорта Кишинева и требует миллион рублей, сообщили в пограничной полиции страны. Из -здания эвакуированы пассажиры и персонал, авиарейсы задерживаются.Сотрудники всех\n",
      "Corpus data (words): ['неизвестный', 'угрожает', 'взорвать', 'аэропорт', 'кишинева', ',', 'если', 'ему', 'не', 'дадут', 'миллион', '.', 'неизвестный', 'сообщил', 'о', 'бомбе', 'в', 'аэропорту', 'международного', 'аэропорта', 'кишинева', 'и', 'требует', 'миллион', 'рублей', ',', 'сообщили', 'в', 'пограничной', 'полиции', 'страны', '.', 'из', 'здания', 'эвакуированы', 'пассажиры', 'и', 'персонал', ',', 'авиарейсы', 'задерживаются', '.', 'сотрудники', 'всех', 'подразделений', 'мвд', 'участвуют', 'в', 'операции', 'в']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpora_paths = ['./articles/good.articles', './articles/bad.articles'] \n",
    "corpora = []\n",
    "lengths = []\n",
    "\n",
    "for path in corpora_paths:\n",
    "  corpus = Corpus(path)\n",
    "  corpora.append(corpus)\n",
    "  length = [len(article) for article in corpus.articles]\n",
    "  lengths.append(length)\n",
    "\n",
    "  print(f\"Corpus label: {corpus.label}, \",\n",
    "        f\"length: {len(corpus.articles)} articles, \",\n",
    "        f\"av length: {round(corpus.average_length())} words, \",\n",
    "        f\"max length: {max(length)} words.\")\n",
    "  print(f\"Corpus raw article: {corpus.raw[0][:300]}\")\n",
    "  print(f\"Corpus data (words): {corpus.articles[0][:50]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning articles by their lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins: 52\n",
      "[{4: 6}, {31: 7}, {85: 8}, {225: 9}, {355: 10}, {525: 11}, {603: 12}, {724: 13}, {711: 14}, {619: 15}, {562: 16}, {536: 17}, {460: 18}, {376: 19}, {300: 20}, {260: 21}, {206: 22}, {166: 23}, {131: 24}, {121: 25}, {98: 26}, {80: 27}, {79: 28}, {42: 29}, {41: 30}, {25: 31}, {31: 32}, {15: 33}, {19: 34}, {14: 35}, {6: 36}, {9: 37}, {13: 38}, {8: 39}, {7: 40}, {2: 41}, {3: 42}, {1: 43}, {4: 44}, {2: 45}, {4: 46}, {2: 48}, {3: 49}, {2: 50}, {1: 51}, {1: 52}, {1: 53}, {1: 57}, {2: 58}, {1: 60}, {1: 82}, {1: 118}]\n",
      "33\n",
      "33\n",
      "34\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# print([i for i, length in enumerate(lengths[0]) if length == maxes[0]])\n",
    "\n",
    "# binning good articles\n",
    "# num_bins = 100\n",
    "bin_size = 5\n",
    "# print(f\"Bin size: {bin_size} words\")\n",
    "\n",
    "# hist, bin_edges = np.histogram(lengths[1], bins=num_bins)\n",
    "\n",
    "bins = {}\n",
    "for article in corpora[1].articles:\n",
    "  length = len(article)\n",
    "  key = length // bin_size \n",
    "  if key in bins.keys():\n",
    "    bins[key].append(article)\n",
    "  else:\n",
    "    bins[key] = [article]\n",
    "\n",
    "print(f\"Number of bins: {len(bins)}\")\n",
    "print([{ len(bins[key]) : key } for key in  sorted(bins.keys())])\n",
    "\n",
    "for article in bins[6]:\n",
    "  print(f\"{len(article)}\")\n",
    "\n",
    "\n",
    "# # visualization\n",
    "# plt.figure().set_size_inches(15, 5)\n",
    "\n",
    "# plt.subplot(211)\n",
    "# plt.title('Lengths distribution: Good articles')\n",
    "# plt.bar(range(len(hist)), hist)\n",
    "\n",
    "# plt.subplots_adjust(hspace=.5)\n",
    "\n",
    "# plt.subplot(212)\n",
    "# plt.title('Lengths distribution: Bad articles')\n",
    "# plt.hist(lengths[1], 100)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top popular words counts: [['UNK', 311786], (',', 191060), ('.', 167323), ('в', 121865), ('number', 70531), ('и', 59437), ('на', 47376), ('с', 26509), ('что', 22411), ('по', 21883)]\n",
      "Example dictionary item: 23732 => слон\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 25000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "# flatten all words into a single bag\n",
    "all_words = [word for corpus in corpora for words in corpus.articles for word in words]\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(all_words)\n",
    "\n",
    "print(f\"Top popular words counts: {count[:10]}\")\n",
    "print(f\"Example dictionary item: {dictionary['слон']} => {reverse_dictionary[23732]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, self._text[self._cursor[b]]] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def words(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  words back into its (most likely) word representation.\"\"\"\n",
    "  return [reverse_dictionary[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  for b in batches:\n",
    "    s = [word for word in words(b)]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_data, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_data, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.float32, shape=[None, vocabulary_size])\n",
    "  train_labels = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      labels=train_labels,\n",
    "      logits=logits\n",
    "    ))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alph = \"abcdefghijklmnoprst\"\n",
    "chars = []\n",
    "\n",
    "for _ in range(10):\n",
    "  r_char = random.choice(list(alph))\n",
    "  chars.append(r_char)\n",
    "\n",
    "print(chars)\n",
    "print(chars[:3])\n",
    "print(chars[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
