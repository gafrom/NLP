{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nlp.cleaner import Cleaner\n",
    "from nlp.tfidfer import TFIDFer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from stored file (113239 bytes)...\n",
      "Counts:\n",
      "1    7518\n",
      "0    6617\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10624</th>\n",
       "      <td>Во Франции цементный концерн подозревают в спо...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>Полиция Ирландии арестовала двух человек после...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>Российские медицинские туристы, которые ездили...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>Следователь московского полицейского главка вы...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>МЧС предупредило москвичей об ухудшении погоды...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    body  label\n",
       "10624  Во Франции цементный концерн подозревают в спо...      1\n",
       "10193  Полиция Ирландии арестовала двух человек после...      1\n",
       "2894   Российские медицинские туристы, которые ездили...      0\n",
       "9087   Следователь московского полицейского главка вы...      1\n",
       "7888   МЧС предупредило москвичей об ухудшении погоды...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STORAGE_PATH = './storage/nb.model'\n",
    "\n",
    "good_articles = pd.read_table('./articles/good.articles', sep='\\n', names=['body'])\n",
    "good_articles['label'] = 0 # good\n",
    "bad_articles  = pd.read_table('./articles/bad.articles', sep='\\n', names=['body'])\n",
    "bad_articles['label'] = 1 # bad\n",
    "\n",
    "# joining\n",
    "articles = pd.concat([good_articles, bad_articles], ignore_index=True)\n",
    "\n",
    "# persistently shuffling\n",
    "index_path = f\"{STORAGE_PATH}.shuffled_index\"\n",
    "if Path(index_path).is_file():\n",
    "  file_size = os.path.getsize(index_path)\n",
    "  print(f\"Loading index from stored file ({file_size} bytes)...\")\n",
    "  with open(index_path, 'rb') as fp:\n",
    "    shuffled_index = pickle.load(fp)\n",
    "else:\n",
    "  print('Shuffling index for the first time ...')\n",
    "  shuffled_index = np.random.permutation(articles.index)\n",
    "  print('Saving index on disk for further access...')\n",
    "  with open(index_path, 'wb') as fp:\n",
    "    pickle.dump(shuffled_index, fp)\n",
    "  file_size = os.path.getsize(index_path)\n",
    "  print(f\"Done. It took {file_size} bytes on the disk.\")\n",
    "\n",
    "articles = articles.reindex(shuffled_index)\n",
    "\n",
    "print(f\"Counts:\\n{articles['label'].value_counts()}\")\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (11308, 2)\n",
      "Test size: (2827, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(articles, train_size = 0.8)\n",
    "\n",
    "print(f\"Train size: {train_data.shape}\")\n",
    "print(f\"Test size: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dictionary and compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 2000\n",
    "cleaner = Cleaner()\n",
    "\n",
    "def build_documents_as_words(documents, cleaner = cleaner):\n",
    "  return [cleaner.words(document) for document in documents['body']]\n",
    "\n",
    "def build_dictionary(documents_as_words, vocabulary_size):\n",
    "  words = [word for words in documents_as_words for word in words]\n",
    "  count = Counter(words).most_common(vocabulary_size)\n",
    "  dictionary = dict()\n",
    "  for word, _ in count: dictionary[word] = len(dictionary)\n",
    "  return dictionary, dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "# building\n",
    "documents_as_words = build_documents_as_words(articles)\n",
    "dictionary, reverse_dictionary = build_dictionary(documents_as_words, vocabulary_size)\n",
    "tfidfer = TFIDFer(dictionary, reverse_dictionary)\n",
    "tf = tfidfer.compute_tf(documents_as_words)\n",
    "\n",
    "feature = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10925    [6.0, 4.0, 3.0, 2.0, 1.0, 5.0, 1.0, 1.0, 0.0, ...\n",
      "5183     [110.0, 62.0, 59.0, 15.0, 40.0, 23.0, 14.0, 9....\n",
      "9870     [5.0, 5.0, 6.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...\n",
      "13631    [6.0, 10.0, 7.0, 4.0, 0.0, 0.0, 0.0, 0.0, 1.0,...\n",
      "9255     [8.0, 5.0, 2.0, 3.0, 2.0, 2.0, 1.0, 1.0, 0.0, ...\n",
      "Name: sparse_body, dtype: object\n",
      "10925    1\n",
      "5183     0\n",
      "9870     1\n",
      "13631    1\n",
      "9255     1\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "def tokenized(text, cleaner = cleaner, dictionary = dictionary):\n",
    "  return [dictionary[word] for word in cleaner.words(text) if word in dictionary]\n",
    "\n",
    "def sparsify(dense, vocabulary_size = vocabulary_size):\n",
    "  sparse = np.zeros(vocabulary_size)\n",
    "  for key in dense: sparse[key] = dense[key]\n",
    "  return sparse\n",
    "\n",
    "train_data['sparse_body'] = train_data['body'].apply(lambda body: sparsify(Counter(tokenized(body))))\n",
    "test_data['sparse_body']  = test_data['body'].apply(lambda body: sparsify(Counter(tokenized(body))))\n",
    "\n",
    "print(train_data['sparse_body'].head())\n",
    "print(train_data['label'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(list(train_data['sparse_body']), list(train_data['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.95      0.97      1312\n",
      "          1       0.96      0.99      0.97      1515\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(list(test_data['sparse_body']))\n",
    "\n",
    "report = metrics.classification_report(test_data['label'], predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
